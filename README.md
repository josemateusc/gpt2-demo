# GPT-2 Baby Toy Example

This repository contains a simple example of training a GPT-2 model to generate text similar to the works of Machado de Assis. The code is designed to demonstrate the basic steps of training a language model using PyTorch. This model has around 10 million parameters and is based on the video by Andrej Karpathy [here](https://youtu.be/kCc8FmEb1nY?si=vUo4UbGV0NuBvOcj). I strongly encourage consuming the content produced by him.

The code is extensively commented, showing the size of inputs and outputs and explaining the purpose of each function to facilitate understanding for beginners.

## Files Description

### `bigram.py`

This script demonstrates a simple bigram model to understand basic concepts of text generation. It includes:

- Data loading and preprocessing using the NLTK Machado corpus.
- Simple bigram model implementation using PyTorch.
- Training and evaluation functions.

### `gpt2Model.py`

This file contains the implementation of a simplified version of the GPT-2 model. Key components include:

- `GPTConfig`: Configuration for the GPT model.
- `GPT`: The GPT model class, which includes the embedding layers, transformer blocks, and the forward method.

### `LLM/dataset.py`

This script includes the `Dataset` class to handle data loading and batching. It processes text data and converts it into a format suitable for training the model.

### `LLM/gpt.py`

Contains the implementation of the core GPT model, including:

- Transformer block definition.
- Multi-head self-attention mechanism.
- Positional encoding.

### `LLM/transformerEncoderOnly.py`

A script focusing on the transformer encoder, used in the GPT model. It includes:

- Implementation of the transformer encoder blocks.
- Layer normalization and feed-forward layers.

### `output.txt`

Contains the output generated by the model after training, showcasing the text generated based on the trained GPT-2 model.

### `output_before_FixBug.txt`

This file contains output from the model before bug fixes were applied, useful for understanding the impact of the fixes on the model's performance.

## Getting Started

### Prerequisites

- Python 3.8 or higher
- PyTorch
- NLTK

### Training the Model

1. Run the bigram model:

   ```bash
   python bigram.py
   ```

2. Train the GPT-2 model:
   ```bash
   python gpt2Model.py
   ```

### Generating Text

After training, you can generate text using the trained model by running the script and examining the `output.txt` file.

## License

This project is licensed under the MIT License.

## Acknowledgements

- This project uses the GPT-2 model architecture inspired by OpenAI's GPT-2.
- The Machado de Assis corpus is provided by NLTK.
- Inspired by Andrej Karpathy's content, which is highly recommended for further learning.
